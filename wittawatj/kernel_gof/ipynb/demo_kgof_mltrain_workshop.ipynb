{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A notebook to demonstrate our new goodness-of-fit test on various models defined on  $\\mathbb{R}$ or $\\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import kgof\n",
    "import kgof.data as data\n",
    "import kgof.density as density\n",
    "import kgof.goftest as gof\n",
    "import kgof.kernel as kernel\n",
    "import kgof.plot as plot\n",
    "import kgof.util as util\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "# import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import kgof.plot\n",
    "kgof.plot.set_default_matplotlib_options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "/* highlight */\n",
    ".hl {\n",
    "    background-color: #eeee00;\n",
    "    padding: 1px 5px 1px 5px;\n",
    "    margin: 4px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Define some convenient functions that we will use many times later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def prob2d_pqgauss(qmean=np.array([0, 0]), QVar=np.eye(2), seed=2):\n",
    "    \"\"\"\n",
    "    Construct a problem where p = N(0, I), q = N(0, QVar).\n",
    "    \n",
    "    Return p, a DataSource (for q)\n",
    "    \"\"\"\n",
    "    p = density.Normal(np.array([0, 0]), np.eye(2))\n",
    "    ds = data.DSNormal(qmean, QVar)\n",
    "    return p, ds\n",
    "\n",
    "def plot2d_pq(p, X, n_max=300, n_xdengrid=50, n_ydengrid=50, \n",
    "            xlim=None, ylim=None, margin=0, V=None, V0=None, \n",
    "              figw=8, figh=5):\n",
    "    \"\"\"\n",
    "    A function to plot the model p, and the sample from q, along with other\n",
    "    information such as the learned test locations V.\n",
    "    \n",
    "    p: UnnormalizedDensity (model)\n",
    "    X: nx2 data matrix\n",
    "    n_max: do not plot the data more than this number of points\n",
    "    n_xdengrid, n_ydengrid: number of points to use to plot the density contour \n",
    "    V: J x 2 matrix of J features (test locations) to show\n",
    "    V0: J x 2 matrix of J features. Initial values before optimization.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(figw, figh))\n",
    "    # plot the data\n",
    "#     plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.7, color='#8282FF', markersize=8);\n",
    "    n = X.shape[0]\n",
    "    n_sub = min(n, n_max)\n",
    "    plt.plot(X[:n_sub, 0], X[:n_sub, 1], 'bo', alpha=0.8, markeredgecolor='white', markersize=6);\n",
    "    xmin, ymin = np.min(X, 0)\n",
    "    xmax, ymax = np.max(X, 0)\n",
    "    if xlim is None:\n",
    "        xlim = [xmin-margin, xmax+margin]\n",
    "    if ylim is None:\n",
    "        ylim = [ymin-margin, ymax+margin]\n",
    "    \n",
    "    \n",
    "    try: \n",
    "        flogden = lambda Xt: p.log_normalized_den(Xt)\n",
    "        # call it to see if it is implemented.\n",
    "        flogden(np.array([[1,2]]))\n",
    "    except NotImplementedError as e:\n",
    "        #print 'Use log of the unnormalized density.'\n",
    "        flogden = lambda Xt: p.log_den(Xt)\n",
    "    # get a mesh to plot the contour of the density p\n",
    "    XX, YY, Logden = plot.box_meshgrid(flogden, xlim, ylim, nx=n_xdengrid, ny=n_ydengrid)\n",
    "    \n",
    "    # plot the unnormalized density\n",
    "    Den = np.exp(Logden)\n",
    "#     list_colors = plt.cm.datad['Reds']\n",
    "#     list_colors = list(list_colors)\n",
    "#     list_colors[-1] = (1, 1, 1)\n",
    "#     lscm = matplotlib.colors.LinearSegmentedColormap(\"my_Reds\", list_colors)\n",
    "\n",
    "    plt.contourf(XX, YY, Den, 7, \n",
    "                cmap=plot.get_density_cmap(),\n",
    "                 alpha = 0.7\n",
    "               );\n",
    "    if V is not None:\n",
    "        # feature\n",
    "        plt.plot(V[:, 0], V[:, 1], 'm*', markeredgecolor='white', \n",
    "                 label='Optimized', markersize=34)\n",
    "    if V0 is not None:\n",
    "        # plot the initial test locations (before the optimization)\n",
    "        plt.plot(V0[:, 0], V0[:, 1], 'k*', markeredgecolor='white', \n",
    "                 label='Initial', markersize=34)\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Interactive 1D mixture model problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def func_fssd_power_criterion(p, X, k, V):\n",
    "    \"\"\"\n",
    "    Return the value of the power criterion of FSSD.\n",
    "    p: model density\n",
    "    X: n x d data matrix\n",
    "    k: a Kernle\n",
    "    V: J x d numpy array. J test locations\n",
    "    \"\"\"\n",
    "    dat = data.Data(X)\n",
    "    return gof.FSSD.power_criterion(p, dat, k, V, reg=1e-5, use_unbiased=False)\n",
    "\n",
    "def plot1d_pq(p, X, func_obj=None, rescale_obj=False, xlim=None, n_dom=200, n_bins=20,\n",
    "              margin=0, V=None, V0=None, \n",
    "              figw=8, figh=5):\n",
    "    \"\"\"\n",
    "    A function to plot the model p, and the sample from q, along with other\n",
    "    information such as the learned test locations V. \n",
    "    Create a 1D plot.\n",
    "    \n",
    "    p: UnnormalizedDensity (model)\n",
    "    func_obj: a function: m x 1 -> m-array for computing the optimization \n",
    "        objective at m locations.\n",
    "    rescale_obj: if true, rescale the objective function to fit the height\n",
    "        of the histogram.\n",
    "    n_dom: number of points to use to plot the density\n",
    "    n_bins: number of bins to use for the histogram\n",
    "    X: nx2 data matrix\n",
    "    V: J x 1 matrix of J features (test locations) to show\n",
    "    V0: J x 1 matrix of J features. Initial values before optimization.\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    assert d==1\n",
    "    assert V is None or V.shape[1] == 1\n",
    "    assert V0 is None or V0.shape[1] == 1\n",
    "    \n",
    "    plt.figure(figsize=(figw, figh))\n",
    "    xmin = np.min(X, 0)\n",
    "    xmax = np.max(X, 0)\n",
    "    if xlim is None:\n",
    "        xlim = [xmin-margin, xmax+margin]\n",
    "    \n",
    "    # plot the data as a histogram\n",
    "    hist_result = plt.hist(X, normed=True, bins=n_bins, alpha=0.8, label='Data')\n",
    "    hist_heights = hist_result[0]\n",
    "    hist_parts = hist_result[1]\n",
    "    \n",
    "    try: \n",
    "        is_normalized = True\n",
    "        flogden = lambda Xt: p.log_normalized_den(Xt)\n",
    "        # call it to see if it is implemented.\n",
    "        flogden(np.array([[1]]))\n",
    "    except NotImplementedError as e:\n",
    "        #print 'Use log of the unnormalized density.'\n",
    "        flogden = lambda Xt: p.log_den(Xt)\n",
    "        is_normalized = False\n",
    "        \n",
    "    dom = np.linspace(xlim[0], xlim[1], 200)\n",
    "    den = np.exp(flogden(dom[:, np.newaxis]))\n",
    "    if not is_normalized:\n",
    "        # If p is an unnormalized density, then \n",
    "        # normalize it to fit the height of the histogram\n",
    "        # (this is not technically correct. This is just a \n",
    "#         simple way to plot.)\n",
    "        hist_height = np.max(hist_heights)\n",
    "        max_den = np.max(den)\n",
    "        # renormalize\n",
    "        den = den/max_den*hist_height\n",
    "     \n",
    "    # plot the density (may be unnormalized)\n",
    "    plt.plot(dom, den, 'r-', label=r'$p$')\n",
    "    \n",
    "    if func_obj is not None:\n",
    "        # plot the optimization objective\n",
    "        objs = func_obj(dom[:, np.newaxis])\n",
    "        if rescale_obj:\n",
    "            # rescale to match the height of the histogram\n",
    "            max_obj = np.max(objs)\n",
    "            objs = objs/max_obj*hist_height*0.8\n",
    "        plt.plot(dom, objs, 'k-', label=r'Score function')\n",
    "    \n",
    "    if V is not None:\n",
    "        # feature\n",
    "        plt.plot(V[:, 0], V[:, 1], 'm*', markeredgecolor='white', \n",
    "                 label='Optimized', markersize=34)\n",
    "    if V0 is not None:\n",
    "        # plot the initial test locations (before the optimization)\n",
    "        plt.plot(V0[:, 0], V0[:, 1], 'k*', markeredgecolor='white', \n",
    "                 label='Initial', markersize=34)\n",
    "    plt.xlim(xlim)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def prob_1dmixture(pm=0, pv=1, qm1=0, qv1=1, qm2=1, qv2=1, seed=3):\n",
    "    \"\"\"\n",
    "    A 1D problem where both p and q are 2-component Gaussian mixture models.\n",
    "    p(x) = 0.5*N(0, 1) + 0.5*N(pm, pv)\n",
    "    q(x) = 0.5*N(qm1, qv1) + 0.5*N(qm2, qv2)\n",
    "    \n",
    "    Return p and q (both are UnnormalizedDensity)\n",
    "    \"\"\"\n",
    "    assert pv > 0\n",
    "    assert qv1 > 0\n",
    "    assert qv2 > 0\n",
    "    p = density.IsoGaussianMixture(means=np.array([[0], [pm]]), \n",
    "                                  variances=np.array([1, pv]))\n",
    "    q = density.IsoGaussianMixture(means=np.array([[qm1], [qm2]]),\n",
    "                                  variances=np.array([qv1, qv2]))\n",
    "    return p, q\n",
    "\n",
    "def func_interactive_1dmixture(pm=0, pv=1, qm1=0, qv1=1, qm2=1, qv2=1):\n",
    "    \n",
    "    seed = 84\n",
    "    p, q = prob_1dmixture(pm=pm, pv=pv, qm1=qm1, qv1=qv1, qm2=qm2,\n",
    "                          qv2=qv2, seed=seed)\n",
    "    \n",
    "    # n = sample size to draw from q\n",
    "    n = 600\n",
    "    gwidth2 = 1.5**2\n",
    "    # generate data from q\n",
    "    ds = q.get_datasource()\n",
    "    dat = ds.sample(n, seed=seed+3)\n",
    "    Xs = dat.data()\n",
    "    # kernel\n",
    "    k = kernel.KGauss(sigma2=gwidth2)\n",
    "    def score_function(Vs):\n",
    "        \"\"\"\n",
    "        Vs: m x d test locations. \n",
    "        Evaluate the score at m locations\n",
    "        \"\"\"\n",
    "        m = Vs.shape[0]\n",
    "        objs = np.zeros(m)\n",
    "        for i in range(m):\n",
    "            v = Vs[i, :]\n",
    "            obj = func_fssd_power_criterion(p, Xs, k, v[np.newaxis, :])\n",
    "            objs[i] = obj\n",
    "        return objs\n",
    "    \n",
    "    # plot the problem\n",
    "    plot1d_pq(p, Xs, func_obj=score_function, rescale_obj=False,\n",
    "              margin=3, n_dom=100, n_bins=12, figw=8, figh=5)\n",
    "    print('p = 0.5*N(0, 1) + 0.5*N({}, {})'.format(pm, pv))\n",
    "    print('q = 0.5*N({}, {}) + 0.5*N({}, {})'.format(qm1, qv1, qm2, qv2))\n",
    "    plt.legend(loc='upper right', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "vs = interactive(func_interactive_1dmixture, pm=(0, 10, 1), pv=(1e-3, 5, 1),\n",
    "                 qm1=(-5, 10, 1), qv1=(1e-3, 5, 1), qm2=(-5, 10, 1),\n",
    "                 qv2=(1e-3, 5, 1)\n",
    "                )\n",
    "display(vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goodness-of-fit test\n",
    "\n",
    "Given a known probability density $p$ (model) and a sample $\\{ \\mathbf{x}_i \\}_{i=1}^n \\sim q$ where $q$ is an unknown density, a goodness-of-fit test proposes the null hypothesis\n",
    "\n",
    "<span class=\"hl\">$H_0: p = q$</span>\n",
    "\n",
    "against the alternative hypothesis\n",
    "\n",
    "$H_1: p \\neq q$. \n",
    "\n",
    "In other words, it tests whether or not the sample $\\{ \\mathbf{x}_i \\}_{i=1}^n $ is distributed according to a known  $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A Gaussian mean shift problem\n",
    "\n",
    "Let us assume that the model\n",
    "\n",
    "$$p(\\mathbf{x})=\\mathcal{N}\\left(\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "0\n",
    "\\end{array}\\right],\\left[\\begin{array}{cc}\n",
    "1 & 0\\\\\n",
    "0 & 1\n",
    "\\end{array}\\right]\\right).\n",
    "$$\n",
    "\n",
    "in $\\mathbb{R}^2$ (two-dimensional space). \n",
    "\n",
    "## Construct the model $p$\n",
    "\n",
    "The model $p$ is specified by its density function. Specifically, our test requires $\\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$ (i.e., the derivative of the log density) which does not depend on the normalizer. So, when specifying the density $p$, it is sufficient to specify it up to the normalizer. The gradient $\\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$ will be automatically computed by `autograd`. Alternatively, we can also directly specify $\\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$. Here, we will construct our model by specifying $p(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Assume that we do the following import\n",
    "\n",
    "`import kgof.density as density`.\n",
    "\n",
    "In `kgof` package, a model $p$ can be specified by implementing the class `density.UnnormalizedDensity`. Implementing this directly is a bit tedious, however. An easier way is to use the function \n",
    "\n",
    "<span class=\"hl\">    density.from_log_den(d, f) </span>\n",
    "    \n",
    "which takes as input 2 arguments:\n",
    "\n",
    "1. `d`: the dimension of the input space\n",
    "2. `f`: a function taking in a 2D numpy array of size `n x d` and producing a one-dimensional array of size `n` for the `n` values of the log unnormalized density.\n",
    "\n",
    "Let us construct an `UnnormalizedDensity` which is the object representing a model. All the implemented goodness-of-fit tests take this object as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Assume two dimensions.\n",
    "d = 2\n",
    "def isogauss_log_den(X):\n",
    "    \"\"\"\n",
    "    Evaluate the log density of the standard isotropic Gaussian \n",
    "    at the points (rows) in X.\n",
    "    Note that the density is NOT normalized. \n",
    "    \n",
    "    X: n x d nd-array\n",
    "    return a length-n array\n",
    "    \"\"\"\n",
    "    # d = dimension of the input space\n",
    "    unden = -np.sum(X**2, 1)/2.0\n",
    "    return unden\n",
    "\n",
    "# p is an UnnormalizedDensity object\n",
    "p = density.from_log_den(d, isogauss_log_den)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generate data from $q$\n",
    "\n",
    "In practice, the data $\\{ \\mathbf{x}_i \\}_{i=1}^n $ are given to us. However, here as an illustrative example, we will generate them from $q$ which is\n",
    "\n",
    "$$q(\\mathbf{x})=\\mathcal{N}\\left(\\left[\\begin{array}{c}\n",
    "m\\\\\n",
    "0\n",
    "\\end{array}\\right],\\left[\\begin{array}{cc}\n",
    "1 & 0\\\\\n",
    "0 & 1\n",
    "\\end{array}\\right]\\right),\n",
    "$$\n",
    "\n",
    "\n",
    "where $m$ specifies the mean of the first coordinate of $q$. From this setting, if $m\\neq 0$, then $H_1$ is true and the test should reject $H_0$. If $m=0$, then $p=q$, and the test should not reject $H_0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's assume that m = 1.\n",
    "m = 1\n",
    "\n",
    "# Draw n points from q\n",
    "seed = 5\n",
    "np.random.seed(seed)\n",
    "n = 300\n",
    "X = np.random.randn(n, 2) + np.array([m, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Plot the data and model $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot2d_pq(p, X)\n",
    "plt.axis('equal');\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "All the implemented tests take the data in the form of a `data.Data` object. This is just an encapsulation of the sample `X`. To construct `data.Data` we do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# dat will be fed to the test.\n",
    "dat = data.Data(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization of test locations\n",
    "\n",
    "Now that we have the data, let us randomly split it into two disjoint halves: `tr` and `te`. The training set `tr` will be used for parameter optimization. The testing set `te` will be used for the actual goodness-of-fit test. `tr` and `te` are again of type `data.Data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We will use some portion of the data for parameter tuning, and the rest for testing.\n",
    "tr, te = dat.split_tr_te(tr_proportion=0.5, seed=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us optimize the parameters of the test on `tr`. The optimization relies on `autograd` to compute the gradient. We will use a Gaussian kernel for the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# J is the number of test locations (or features). Typically not larger than 10.\n",
    "J = 1\n",
    "\n",
    "# There are many options for the optimization. \n",
    "# Almost all of them have default values. \n",
    "# Here, we will list a few to give you a sense of what you can control.\n",
    "# Full options can be found in gof.GaussFSSD.optimize_locs_widths(..)\n",
    "opts = {\n",
    "    'reg': 1e-2, # regularization parameter in the optimization objective\n",
    "    'max_iter': 50, # maximum number of gradient ascent iterations\n",
    "    'tol_fun':1e-4, # termination tolerance of the objective\n",
    "}\n",
    "\n",
    "# make sure to give tr (NOT te).\n",
    "# do the optimization with the options in opts.\n",
    "V_opt, gw_opt, opt_info = gof.GaussFSSD.optimize_auto_init(p, tr, J, **opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The optimization procedure returns back \n",
    "\n",
    "1. `V_opt`: optimized test locations (features). A $J \\times d$ numpy array.\n",
    "2. `gw_opt`: optimized Gaussian width (for the Gaussian kernel). A floating point number.\n",
    "3. `opt_info`: a dictionary containing information gathered during the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "opt_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goodness-of-fit test\n",
    "\n",
    "Let us use these optimized parameters to construct the FSSD test. Our test using a Gaussian kernels is implemented in `kgof.goftest.GaussFSSD`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alpha = significance level of the test\n",
    "alpha = 0.01\n",
    "fssd_opt = gof.GaussFSSD(p, gw_opt, V_opt, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a dictionary of testing results\n",
    "test_result = fssd_opt.perform_test(te)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the null hypothesis was $H_0: p=q$. It can be seen that the test correctly <span class=\"hl\">rejects $H_0$ with a very small p-value.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learned test location $\\mathbf{v}$ \n",
    "\n",
    "Let us check the optimized test location(s). We will plot the training data, the learned feature(s) and the contour of the unnormalized <span class=\"hl\">density $p$ in red</span>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = tr.data()\n",
    "plot2d_pq(p, Xtr, V=V_opt, margin=0.5,\n",
    "#         , xlim=[-4, 4]\n",
    "       )\n",
    "plt.axis('equal');\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise \n",
    "Go back to where we sample the data from $q$, and <span class=\"hl\">change `m` to 0</span> (i.e., the mean of the first coordinate of $q$) . This will make $p=q$ so that $H_0$ is now true. Run the whole procedure again and verify that the test will not reject $H_0$. (Technically, the probability of rejecting is about $\\alpha$.) \n",
    "\n",
    "Note that when the test fails to reject, the learned features are not interpretable. They will be arbitrary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian problem. Variance difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_params(Xtr, J, seed=40):\n",
    "    \"\"\"\n",
    "    Return V0, gwidth0 where V0 is a J x d numpy array containing\n",
    "    J initial test locations before optimization. These are initialized\n",
    "    by drawing from a Gaussian fitted to the training set.\n",
    "    gwidth0 is the initial Gaussian width initialized by \n",
    "    the median heuristic.\n",
    "    \n",
    "    Xtr: n x d numpy array of n data points\n",
    "    J: number of test locations required\n",
    "    \"\"\"\n",
    "    \n",
    "    # random test locations\n",
    "    V0 = util.fit_gaussian_draw(Xtr, J, seed=seed+11)\n",
    "    gwidth0 = util.meddistance(Xtr, subsample=1000)**2\n",
    "    return V0, gwidth0\n",
    "\n",
    "\n",
    "def optimize_params(p, tr, V0, gwidth0):\n",
    "    \"\"\"\n",
    "    Optimize the test locations and the Gaussian width on the trainig data.\n",
    "    \n",
    "    p: the model. UnnormalizedDensity.\n",
    "    tr: data.Data object representing the training set\n",
    "    V0: J x d numpy array of J initial test locations\n",
    "    gwidth0: initial Gaussian width\n",
    "    opts: a dictionary containing options to the optimizer\n",
    "    \n",
    "    Return V_opt, gw_opt, opt_info\n",
    "    \"\"\"\n",
    "    # There are many options for the optimization. \n",
    "    # Almost all of them have default values. \n",
    "    # Here, we will list a few to give you a sense of what you can control.\n",
    "    # Full options can be found in gof.GaussFSSD.optimize_locs_widths(..)\n",
    "    opts = {\n",
    "        'reg': 1e-2, # regularization parameter in the optimization objective\n",
    "        'max_iter': 50, # maximum number of gradient ascent iterations\n",
    "        'tol_fun':1e-5, # termination tolerance of the objective\n",
    "        'gwidth_lb': 1**2, #absolute lower bound on the Gaussian width^2\n",
    "        'gwidth_ub': 10**2,\n",
    "    }\n",
    "\n",
    "    # make sure to give tr (NOT te).\n",
    "    # do the optimization with the options in opts.\n",
    "    # V_opt, gw_opt, opt_info = gof.GaussFSSD.optimize_auto_init(p, tr, J, **opts)\n",
    "    V_opt, gw_opt, opt_info = gof.GaussFSSD.optimize_locs_widths(p, tr, gwidth0, V0, **opts)\n",
    "    return V_opt, gw_opt, opt_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Gaussian variance difference problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p is an UnnormalizedDensity object\n",
    "seed = 30\n",
    "n = 1500\n",
    "p, ds = prob2d_pqgauss(qmean=np.array([0, 0]), QVar=np.diag([2, 0.5]), seed=seed)\n",
    "dat = ds.sample(n, seed=seed+2)\n",
    "X = dat.data()\n",
    "\n",
    "# plot\n",
    "plot2d_pq(p, X)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use a portion (tr_proportion) of the data for parameter tuning\n",
    "tr, te = dat.split_tr_te(tr_proportion=0.3, seed=seed+8)\n",
    "Xtr = tr.data()\n",
    "\n",
    "# J is the number of test locations (or features). Typically not larger than 10.\n",
    "J = 2\n",
    "V0, gwidth0 = init_params(Xtr, J, seed=seed+3)\n",
    "V_opt, gw_opt, opt_info = optimize_params(p, tr, V0, gwidth0)\n",
    "opt_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness-of-fit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = significance level of the test\n",
    "alpha = 0.01\n",
    "fssd_opt = gof.GaussFSSD(p, gw_opt, V_opt, alpha)\n",
    "\n",
    "# Goodness-of-fit test\n",
    "# return a dictionary of testing results\n",
    "test_result = fssd_opt.perform_test(te)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot2d_pq(p, Xtr, V0=V0, V=V_opt, margin=0,\n",
    "#         , xlim=[-4, 4]\n",
    "       )\n",
    "plt.axis('equal');\n",
    "plt.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A complicated 2D mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k x d where k = number of mixture components\n",
    "pmeans = np.array([[0, 0], [3, 3], [3, 0]])\n",
    "pvariances = np.ones(3)*1\n",
    "p = density.IsoGaussianMixture(pmeans, pvariances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# q is a Gaussian mixture\n",
    "qmeans = np.array([[0, 0], [3, 3], [0, 3]])\n",
    "qvariances = pvariances\n",
    "q = density.IsoGaussianMixture(qmeans, qvariances)\n",
    "ds = q.get_datasource()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate some data from q\n",
    "n = 800\n",
    "dat = ds.sample(n, seed=seed+23)\n",
    "X = dat.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2d_pq(p, X, margin=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use a portion (tr_proportion) of the data for parameter tuning\n",
    "tr, te = dat.split_tr_te(tr_proportion=0.5, seed=seed+8)\n",
    "Xtr = tr.data()\n",
    "\n",
    "# J is the number of test locations (or features). Typically not larger than 10.\n",
    "J = 2\n",
    "V0, gwidth0 = init_params(Xtr, J, seed=seed+3)\n",
    "V_opt, gw_opt, opt_info = optimize_params(p, tr, V0, gwidth0)\n",
    "opt_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = significance level of the test\n",
    "alpha = 0.01\n",
    "fssd_opt = gof.GaussFSSD(p, gw_opt, V_opt, alpha)\n",
    "\n",
    "# Goodness-of-fit test\n",
    "# return a dictionary of testing results\n",
    "test_result = fssd_opt.perform_test(te)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2d_pq(p, Xtr, V0=V0, V=V_opt, margin=0,\n",
    "#         , xlim=[-4, 4]\n",
    "       )\n",
    "plt.axis('equal');\n",
    "plt.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "V_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Complicated model: 1D Restricted Boltzmann Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob_rbm_perturb(B_scale=1, b_scale=1, c_scale=1, \n",
    "                     B_perturb=0, b_perturb=0, c_perturb=0,\n",
    "                    d=1, dh=8, seed=38):\n",
    "    \"\"\"\n",
    "    B_scale, b_scale, c_scale: control scaling of the parameters of the RBMs\n",
    "    B_perturb, b_perturb, c_perturb: how much q differs from p\n",
    "    d: dimension of the input\n",
    "    dh: dimension of the hidden variables\n",
    "    \n",
    "    Return p and q (both are UnnormalizedDensity)\n",
    "    \"\"\"\n",
    "    assert B_scale >= 0\n",
    "    assert b_scale >= 0\n",
    "    assert c_scale >= 0\n",
    "    assert B_perturb >= 0\n",
    "    assert b_perturb >= 0\n",
    "    assert c_perturb >= 0\n",
    "    \n",
    "    with util.NumpySeedContext(seed=seed):\n",
    "    #     B = np.array([-1, 1])[np.random.randint(0, 2, (d, dh))]\n",
    "        B = np.random.randn(d, dh)*B_scale\n",
    "        b = np.random.rand(d)*b_scale - b_scale/2.0\n",
    "        c = np.random.rand(dh)*c_scale - c_scale/2.0\n",
    "\n",
    "        # the model p\n",
    "        p = density.GaussBernRBM(B, b, c)\n",
    "\n",
    "        # perturb parameters of p to construct q\n",
    "        Bq = B + np.random.randn(d, dh)*B_perturb\n",
    "        bq = b + np.random.randn(d)*b_perturb\n",
    "        cq = c + np.random.randn(dh)*c_perturb\n",
    "        # construct the density q\n",
    "        q = density.GaussBernRBM(Bq, bq, cq)\n",
    "    return p, q\n",
    "\n",
    "def func_interactive_rbm_problem(B_scale=1, b_scale=1, c_scale=1, \n",
    "                     B_perturb=0, b_perturb=0, c_perturb=0):\n",
    "    dh = 7\n",
    "    d = 1\n",
    "    seed = 84\n",
    "    # n = sample size to draw from q\n",
    "    n = 500\n",
    "    gwidth2 = 3**2\n",
    "    \n",
    "    p, q = prob_rbm_perturb(B_scale=B_scale, b_scale=b_scale, c_scale=c_scale, \n",
    "                     B_perturb=B_perturb, b_perturb=b_perturb, c_perturb=c_perturb,\n",
    "                    d=d, dh=dh, seed=seed)\n",
    "    \n",
    "    # generate data from q\n",
    "    ds = q.get_datasource()\n",
    "    # Sampling from the RBM relies on Gibbs. Just to make it cheaper,\n",
    "    # let us set the burnin iterations to be reasonably small.\n",
    "    ds.burnin = 1000\n",
    "    dat = ds.sample(n, seed=seed+37)\n",
    "    Xs = dat.data()\n",
    "    # kernel\n",
    "    k = kernel.KGauss(sigma2=gwidth2)\n",
    "    def score_function(Vs):\n",
    "        \"\"\"\n",
    "        Vs: m x d test locations. \n",
    "        Evaluate the score at m locations\n",
    "        \"\"\"\n",
    "        m = Vs.shape[0]\n",
    "        objs = np.zeros(m)\n",
    "        for i in range(m):\n",
    "            v = Vs[i, :]\n",
    "            obj = func_fssd_power_criterion(p, Xs, k, v[np.newaxis, :])\n",
    "            objs[i] = obj\n",
    "        return objs\n",
    "    \n",
    "    # plot the problem\n",
    "    plot1d_pq(p, Xs, func_obj=score_function, rescale_obj=False,\n",
    "              margin=1, n_dom=100, n_bins=12, figw=10, figh=6)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "vs = interactive(func_interactive_rbm_problem, B_scale=(0, 2, 0.1), \n",
    "                 b_scale=(0, 3, 0.2), c_scale=(0, 3, 0.2), \n",
    "                     B_perturb=(0, 1, 0.05),\n",
    "                 b_perturb=fixed(0), c_perturb=fixed(0)\n",
    "                )\n",
    "display(vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the widget above, `B_scale`, `b_scale`, and `c_scale` controls the parameters of the model $p$. $q$ is the same as $p$ unless `B_perturb` is not 0 i.e., the matrix parameter in $p$ is perturbed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
